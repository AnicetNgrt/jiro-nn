{
    "name": "D",
    "learning_rate": 0.05,
    "decay": 1.0,
    "epochs": 30,
    "dropout": [0.1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
    "batch_size": 132,
    "activation": "sigm",
    "layers": [36, 36, 36, 36, 36, 36, 36]
}